{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d32bc496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from openlocationcode import openlocationcode as olc\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "127ff7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成！\n"
     ]
    }
   ],
   "source": [
    "# 数据过滤\n",
    "\n",
    "# 处理空间位置\n",
    "def get_pluscode(latitude, longitude):\n",
    "    if pd.isna(latitude) or pd.isna(longitude):\n",
    "        return \"INVALID\"\n",
    "    try:\n",
    "        code = olc.encode(float(latitude), float(longitude))\n",
    "        return code[:6]\n",
    "    except:\n",
    "        return \"INVALID\"\n",
    "\n",
    "\n",
    "# 处理时间\n",
    "def format_time(row):\n",
    "    # 示例输入: \"Mon Oct 19 20:06:23 +0800 2025\" — 但 tz_offset 已单独给出\n",
    "    # 忽略字符串中的时区，只用 tz_offset\n",
    "    time_str = row['time']\n",
    "    # 拆分出日期部分和年份\n",
    "    parts = time_str.split()\n",
    "    # 重建为 \"Oct 19 20:06:23 2025\"\n",
    "    clean_time_str = f\"{parts[1]} {parts[2]} {parts[3]} {parts[5]}\"\n",
    "    naive_dt = datetime.strptime(clean_time_str, \"%b %d %H:%M:%S %Y\")\n",
    "    \n",
    "    # 使用 tz_offset（分钟）创建时区\n",
    "    tz = timezone(timedelta(minutes=int(row['tz_offset'])))\n",
    "    localized_dt = naive_dt.replace(tzinfo=tz)\n",
    "    return localized_dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "# 保持映射关系\n",
    "def save_mapping(mapping, file_path):\n",
    "    # 保存uid映射文件\n",
    "    with open(file_path, \"w\", newline=\"\") as uidfile:\n",
    "        writer = csv.writer(uidfile)\n",
    "        writer.writerow([\"original_uid\", \"new_uid\"])\n",
    "        for original_uid, new_uid in mapping.items():\n",
    "            writer.writerow([original_uid, new_uid])\n",
    "\n",
    "\n",
    "def filter_data(datafold=None, min_user_interactions=10, min_poi_interactions=10):\n",
    "\n",
    "    os.makedirs(f\"{datafold}\", exist_ok=True)\n",
    "\n",
    "    input_file = f\"{datafold}.txt\"\n",
    "    df = pd.read_csv(input_file, delimiter=\"\\t\", header=None,\n",
    "                     names=['uid', 'pid', '_', 'category', 'latitude', 'longitude', 'tz_offset', 'time'])\n",
    "\n",
    "    # 转换经纬度为数值\n",
    "    df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "    df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "    df = df.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "    # 过滤 POI\n",
    "    poi_counts = df['pid'].value_counts()\n",
    "    valid_pids = poi_counts[poi_counts >= min_poi_interactions].index\n",
    "    df = df[df['pid'].isin(valid_pids)]\n",
    "\n",
    "    # 过滤用户\n",
    "    user_counts = df['uid'].value_counts()\n",
    "    valid_uids = user_counts[user_counts >= min_user_interactions].index\n",
    "    df = df[df['uid'].isin(valid_uids)].copy()\n",
    "\n",
    "    # 生成 region\n",
    "    df[\"region\"] = df.apply(lambda row: get_pluscode(row['latitude'], row['longitude']), axis=1)\n",
    "\n",
    "    # 映射 ID\n",
    "    uid_map, pid_map, cid_map, region_map = {}, {}, {}, {}\n",
    "    df['new_uid'] = df['uid'].apply(lambda x: uid_map.setdefault(x, len(uid_map)))\n",
    "    df['new_pid'] = df['pid'].apply(lambda x: pid_map.setdefault(x, len(pid_map)))\n",
    "    df['new_cid'] = df['category'].apply(lambda x: cid_map.setdefault(x, len(cid_map)))\n",
    "    df['new_region'] = df['region'].apply(lambda x: region_map.setdefault(x, len(region_map)))\n",
    "    df['formatted_time'] = df.apply(format_time, axis=1)\n",
    "\n",
    "    # 保存\n",
    "    output_file = f\"{datafold}/{datafold}.csv\"\n",
    "    df[['new_uid', 'new_pid', 'new_cid', 'category', 'new_region', 'latitude', 'longitude', 'formatted_time']].to_csv(\n",
    "        output_file, index=False, header=['uid', 'pid', 'cid', 'category', 'region', 'latitude', 'longitude', 'time']\n",
    "    )\n",
    "\n",
    "    save_mapping(uid_map, f\"{datafold}/uidmap.csv\")\n",
    "    save_mapping(pid_map, f\"{datafold}/pidmap.csv\")\n",
    "    save_mapping(cid_map, f\"{datafold}/cidmap.csv\")\n",
    "\n",
    "    print(\"处理完成！\")\n",
    "\n",
    "# 测试\n",
    "datafold = 'NYC'\n",
    "filter_data(datafold, min_user_interactions=10, min_poi_interactions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "171bb35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功创建 NYC/poi_info.csv，共 5135 个 POI\n"
     ]
    }
   ],
   "source": [
    "# 生成 poi_info.csv 文件\n",
    "def poi_info(datafold):\n",
    "    # 读取 data.csv 文件\n",
    "    file_path = f'{datafold}/{datafold}.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"错误：{file_path} 文件未找到。\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 确保 time 列可解析\n",
    "    df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "    df = df.dropna(subset=['time'])  # 移除无法解析的时间\n",
    "\n",
    "    poi_info_data = []\n",
    "\n",
    "    for pid, group in df.groupby('pid'):\n",
    "        # 取第一条记录的元信息（假设同一 POI 的 category/region/经纬度一致）\n",
    "        row0 = group.iloc[0]\n",
    "        category = row0['category']\n",
    "        region = row0['region']\n",
    "        latitude = row0['latitude']\n",
    "        longitude = row0['longitude']\n",
    "\n",
    "        # 统计每小时访问次数\n",
    "        hours = group['time'].dt.hour\n",
    "        hour_counts = hours.value_counts().to_dict()  # {hour: count}\n",
    "\n",
    "        # 可选：只保留 count > 1 的小时（按你的需求）\n",
    "        filtered_hour_counts = {int(h): int(c) for h, c in hour_counts.items() if c > 1}\n",
    "\n",
    "        # 按访问次数降序排序\n",
    "        sorted_hour_counts = dict(\n",
    "            sorted(filtered_hour_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        )\n",
    "\n",
    "        poi_info_data.append({\n",
    "            'pid': pid,\n",
    "            'category': category,\n",
    "            'region': region,\n",
    "            'latitude': latitude,\n",
    "            'longitude': longitude,\n",
    "            'visit_time_and_count': sorted_hour_counts\n",
    "        })\n",
    "\n",
    "    # 创建 DataFrame 并保存\n",
    "    poi_info_df = pd.DataFrame(poi_info_data)\n",
    "    output_path = f'{datafold}/poi_info.csv'\n",
    "    poi_info_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"成功创建 {output_path}，共 {len(poi_info_df)} 个 POI\")\n",
    "\n",
    "# 测试\n",
    "datafold = 'NYC'\n",
    "poi_info(datafold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a45b38ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split done: 118350 train, 28151 test interactions.\n"
     ]
    }
   ],
   "source": [
    "# 数据切分\n",
    "\n",
    "def remove_users_pois_test(df_train, df_test):\n",
    "    # 仅保留测试集中在训练集中出现的用户和POI\n",
    "    valid_users = set(df_train['uid'].unique())\n",
    "    valid_pois = set(df_train['pid'].unique())\n",
    "    df_test = df_test[\n",
    "        df_test['uid'].isin(valid_users) &\n",
    "        df_test['pid'].isin(valid_pois)\n",
    "    ]\n",
    "    return df_test\n",
    "\n",
    "\n",
    "def split_data(datafold, train_ratio=0.8):\n",
    "    file_name = f\"{datafold}/{datafold}.csv\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    # 确保包含所需列\n",
    "    df = df[['uid', 'pid', 'cid', 'category', 'latitude', 'longitude', 'time']]\n",
    "    \n",
    "    # 转换时间并排序\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df = df.sort_values(by='time').reset_index(drop=True)\n",
    "\n",
    "    # 按比例划分\n",
    "    train_size = int(train_ratio * len(df))\n",
    "    train_df = df.iloc[:train_size].copy()\n",
    "    test_df = df.iloc[train_size:].copy()\n",
    "\n",
    "    # 过滤测试集\n",
    "    test_df = remove_users_pois_test(train_df, test_df)\n",
    "\n",
    "    # 为测试用户保留完整历史（用于序列模型）\n",
    "    test_uids = test_df['uid'].unique()\n",
    "    expanded_test_df = df[df['uid'].isin(test_uids)].copy()\n",
    "\n",
    "    # 保存\n",
    "    train_df.to_csv(f'{datafold}/train_data.csv', index=False)\n",
    "    expanded_test_df.to_csv(f'{datafold}/test_data.csv', index=False)\n",
    "\n",
    "    print(f\"Split done: {len(train_df)} train, {len(test_df)} test interactions.\")\n",
    "\n",
    "# 测试\n",
    "split_data('NYC', train_ratio=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a9058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1785071/279462311.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  check_in_sequences = df.groupby('uid').apply(aggregate).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check-in sequences saved to NYC/train_sequences.csv\n",
      "Check-in sequences saved to NYC/test_sequences.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1785071/279462311.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  check_in_sequences = df.groupby('uid').apply(aggregate).reset_index()\n"
     ]
    }
   ],
   "source": [
    "def generate_check_in_sequences(datafold, datafile):\n",
    "    # 读取CSV文件到pandas DataFrame\n",
    "    df = pd.read_csv(f\"{datafold}/{datafile}.csv\")\n",
    "    # 确保时间列是datetime类型\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    # 按uid分组并聚合数据\n",
    "    def aggregate(group):\n",
    "        group = group.sort_values('time')\n",
    "        pid_list = group['pid'].tolist()\n",
    "        return pd.Series({\n",
    "            'pid_sequence': pid_list,\n",
    "        })\n",
    "    check_in_sequences = df.groupby('uid').apply(aggregate).reset_index()\n",
    "    \n",
    "    # 保存结果\n",
    "    output_file = f\"{datafold}/{datafile.split('_')[0]}_sequences.csv\"\n",
    "    check_in_sequences.to_csv(output_file, index=False)\n",
    "    print(f\"Check-in sequences saved to {output_file}\")\n",
    "\n",
    "# 测试\n",
    "generate_check_in_sequences('NYC', 'train_data')\n",
    "generate_check_in_sequences('NYC', 'test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49fa5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total POIs: 5081\n",
      "Matrix saved to NYC/poi_transition_matrix.npy\n",
      "Transition matrix shape: (5081, 5081)\n",
      "Example: POI 0 -> POI 0 count = 9\n"
     ]
    }
   ],
   "source": [
    "def build_poi_transition_matrix(data_path, save_path=None):\n",
    "    # 1. 读取序列文件\n",
    "    df = pd.read_csv(data_path)\n",
    "    # 还原 pid_sequence 为 list\n",
    "    df['pid_sequence'] = df['pid_sequence'].apply(json.loads)\n",
    "\n",
    "    # 2. 收集所有 POI，确定 M\n",
    "    all_pids = set()\n",
    "    for seq in df['pid_sequence']:\n",
    "        all_pids.update(seq)\n",
    "    all_pids = sorted(all_pids)\n",
    "    pid_to_idx = {pid: idx for idx, pid in enumerate(all_pids)}\n",
    "    M = len(all_pids)\n",
    "    print(f\"Total POIs: {M}\")\n",
    "\n",
    "    # 3. 统计转移频次\n",
    "    # 使用 defaultdict(int) 或直接用 numpy zeros\n",
    "    transition_counts = defaultdict(int)\n",
    "\n",
    "    for seq in df['pid_sequence']:\n",
    "        for i in range(len(seq) - 1):\n",
    "            from_pid = seq[i]\n",
    "            to_pid = seq[i + 1]\n",
    "            # 可选：跳过自环（如 0->0）？根据需求决定\n",
    "            # if from_pid == to_pid: continue\n",
    "            transition_counts[(from_pid, to_pid)] += 1\n",
    "\n",
    "    # 4. 构建矩阵（稠密）\n",
    "    A = np.zeros((M, M), dtype=np.int32)\n",
    "    for (i, j), count in transition_counts.items():\n",
    "        if i in pid_to_idx and j in pid_to_idx:\n",
    "            A[pid_to_idx[i], pid_to_idx[j]] = count\n",
    "\n",
    "    # 5. （可选）保存矩阵和映射\n",
    "    if save_path:\n",
    "        np.save(f\"{save_path}_matrix.npy\", A)\n",
    "        print(f\"Matrix saved to {save_path}_matrix.npy\")\n",
    "\n",
    "    return A, pid_to_idx, all_pids\n",
    "\n",
    "# 使用示例\n",
    "datafold = 'NYC'\n",
    "A, pid_to_idx, all_pids = build_poi_transition_matrix(\n",
    "    data_path=f\"{datafold}/train_sequences.csv\",\n",
    "    save_path=f\"{datafold}/poi_transition\"\n",
    ")\n",
    "print(\"Transition matrix shape:\", A.shape)\n",
    "print(\"Example: POI 0 -> POI 0 count =\", A[pid_to_idx[0], pid_to_idx[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3627f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: POI 0 -> POI 1452 count = 1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fb500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating distances (parallel): 100%|██████████| 5135/5135 [01:12<00:00, 70.72POI/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance matrix calculated and saved to distance.csv (parallel, 2 decimal places)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "datafold = 'TKY'\n",
    "\n",
    "# 加载 POI 数据\n",
    "try:\n",
    "    poi_df = pd.read_csv(f'{datafold}/poi_info.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: poi_info.csv not found. Please ensure it's in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "coords = poi_df[['latitude', 'longitude']].to_numpy()\n",
    "poi_ids = poi_df['pid'].tolist()\n",
    "num_pois = len(poi_df)\n",
    "\n",
    "def calculate_distances(i):\n",
    "    distances = {}\n",
    "    for j in range(num_pois):\n",
    "        if i == j:\n",
    "            distances[poi_ids[j]] = 0.0\n",
    "        else:\n",
    "            distance = geodesic(coords[i], coords[j]).km\n",
    "            distances[poi_ids[j]] = round(distance, 2)  # 保留两位小数\n",
    "    return poi_ids[i], distances\n",
    "\n",
    "\n",
    "num_processes = 128  # 根据你的 CPU 核心数调整\n",
    "pool = Pool(processes=num_processes)\n",
    "results = []\n",
    "with tqdm(total=num_pois, desc=\"Calculating distances (parallel)\", unit=\"POI\") as pbar:\n",
    "    for pid, distances in pool.imap_unordered(calculate_distances, range(num_pois)):\n",
    "        results.append((pid, distances))\n",
    "        pbar.update(1)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# 将结果转换为 DataFrame\n",
    "distance_matrix_data = {}\n",
    "for pid, distances in results:\n",
    "    distance_matrix_data[pid] = distances\n",
    "\n",
    "distance_matrix_df = pd.DataFrame.from_dict(distance_matrix_data, orient='index', columns=poi_ids)\n",
    "\n",
    "# 确保列的顺序与索引一致\n",
    "distance_matrix_df = distance_matrix_df.reindex(columns=poi_ids)\n",
    "\n",
    "# 保存距离矩阵到 CSV 文件\n",
    "distance_matrix_df.to_csv(f'{datafold}/distance.csv')\n",
    "\n",
    "print(\"Distance matrix calculated and saved to distance.csv (parallel, 2 decimal places)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d56e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "距离 PID 0 小于 2.0 的 POI PID 有723个:\n",
      "['0', '8', '20', '28', '62', '76', '77', '83', '85', '88', '91', '112', '119', '120', '121', '132', '134', '146', '152', '153', '181', '186', '194', '197', '206', '207', '208', '220', '222', '234', '237', '247', '252', '253', '256', '259', '260', '277', '288', '289', '292', '310', '311', '318', '319', '339', '344', '356', '363', '365', '366', '370', '371', '375', '383', '385', '387', '391', '395', '400', '401', '410', '414', '417', '421', '427', '428', '458', '460', '470', '493', '494', '512', '515', '516', '530', '537', '545', '552', '563', '574', '580', '651', '652', '664', '674', '675', '686', '689', '708', '732', '760', '763', '789', '799', '810', '817', '821', '831', '845', '848', '851', '854', '857', '858', '871', '875', '876', '882', '890', '905', '910', '912', '915', '918', '926', '939', '956', '960', '966', '975', '978', '983', '986', '991', '994', '1002', '1004', '1020', '1021', '1022', '1025', '1038', '1048', '1067', '1068', '1069', '1074', '1086', '1090', '1100', '1104', '1109', '1112', '1115', '1116', '1118', '1119', '1129', '1140', '1145', '1151', '1153', '1154', '1155', '1161', '1168', '1172', '1175', '1178', '1179', '1188', '1192', '1196', '1198', '1200', '1201', '1202', '1204', '1222', '1227', '1229', '1233', '1234', '1235', '1270', '1279', '1280', '1285', '1290', '1291', '1292', '1306', '1312', '1316', '1319', '1322', '1332', '1333', '1340', '1341', '1343', '1346', '1382', '1383', '1385', '1396', '1417', '1458', '1488', '1509', '1512', '1520', '1528', '1530', '1533', '1534', '1537', '1546', '1553', '1555', '1556', '1557', '1563', '1570', '1572', '1578', '1589', '1596', '1599', '1602', '1627', '1635', '1649', '1659', '1662', '1664', '1670', '1671', '1679', '1680', '1701', '1720', '1726', '1731', '1736', '1739', '1745', '1747', '1752', '1754', '1761', '1764', '1767', '1776', '1777', '1778', '1779', '1784', '1786', '1807', '1819', '1823', '1832', '1848', '1881', '1885', '1887', '1888', '1894', '1903', '1904', '1912', '1913', '1923', '1928', '1941', '1945', '1947', '1965', '1967', '1974', '1978', '1979', '1981', '1990', '1995', '2022', '2045', '2046', '2049', '2062', '2065', '2076', '2077', '2085', '2110', '2117', '2139', '2152', '2164', '2171', '2176', '2180', '2183', '2195', '2196', '2213', '2214', '2232', '2234', '2272', '2274', '2288', '2291', '2294', '2306', '2311', '2315', '2318', '2320', '2326', '2334', '2346', '2354', '2357', '2359', '2384', '2387', '2388', '2391', '2399', '2400', '2410', '2418', '2422', '2424', '2442', '2447', '2449', '2451', '2452', '2453', '2455', '2457', '2468', '2477', '2478', '2484', '2489', '2494', '2497', '2503', '2506', '2510', '2519', '2521', '2527', '2537', '2540', '2541', '2545', '2551', '2552', '2564', '2570', '2576', '2577', '2589', '2595', '2613', '2617', '2627', '2630', '2644', '2645', '2667', '2668', '2669', '2673', '2678', '2679', '2683', '2686', '2690', '2694', '2695', '2707', '2708', '2711', '2714', '2715', '2726', '2729', '2743', '2747', '2749', '2756', '2761', '2765', '2790', '2805', '2818', '2840', '2851', '2856', '2859', '2863', '2865', '2866', '2869', '2873', '2877', '2878', '2883', '2888', '2894', '2897', '2898', '2905', '2909', '2913', '2914', '2918', '2926', '2939', '2940', '2955', '2961', '2964', '2971', '2972', '2982', '3023', '3026', '3029', '3036', '3038', '3043', '3045', '3046', '3048', '3068', '3077', '3078', '3079', '3080', '3082', '3088', '3090', '3094', '3097', '3098', '3121', '3131', '3132', '3135', '3143', '3147', '3148', '3151', '3156', '3177', '3179', '3195', '3207', '3208', '3220', '3222', '3224', '3231', '3232', '3233', '3235', '3241', '3246', '3248', '3249', '3254', '3272', '3274', '3284', '3286', '3291', '3295', '3296', '3297', '3304', '3308', '3319', '3320', '3322', '3326', '3330', '3338', '3340', '3342', '3346', '3350', '3364', '3365', '3366', '3368', '3371', '3377', '3378', '3396', '3398', '3401', '3407', '3409', '3416', '3418', '3428', '3434', '3435', '3444', '3468', '3475', '3482', '3494', '3499', '3507', '3511', '3517', '3533', '3537', '3539', '3549', '3551', '3562', '3567', '3569', '3580', '3595', '3597', '3598', '3600', '3606', '3607', '3615', '3621', '3633', '3640', '3650', '3656', '3660', '3666', '3669', '3670', '3672', '3677', '3679', '3682', '3686', '3691', '3705', '3709', '3714', '3716', '3722', '3728', '3730', '3745', '3757', '3760', '3761', '3770', '3771', '3774', '3777', '3779', '3782', '3786', '3788', '3791', '3793', '3797', '3798', '3804', '3824', '3832', '3850', '3853', '3855', '3859', '3860', '3864', '3865', '3869', '3873', '3876', '3887', '3902', '3905', '3925', '3937', '3951', '3954', '3964', '3966', '3969', '3973', '3975', '3982', '3984', '3988', '3992', '3993', '4004', '4006', '4015', '4023', '4031', '4052', '4060', '4089', '4092', '4097', '4100', '4102', '4106', '4114', '4122', '4123', '4134', '4135', '4151', '4152', '4154', '4165', '4176', '4177', '4183', '4185', '4191', '4198', '4201', '4210', '4212', '4232', '4254', '4260', '4261', '4264', '4266', '4267', '4269', '4286', '4289', '4291', '4303', '4307', '4316', '4319', '4321', '4322', '4325', '4335', '4349', '4352', '4378', '4383', '4398', '4399', '4401', '4407', '4411', '4422', '4431', '4433', '4457', '4481', '4509', '4513', '4514', '4515', '4516', '4520', '4527', '4535', '4539', '4548', '4549', '4553', '4561', '4582', '4605', '4611', '4634', '4640', '4643', '4657', '4664', '4684', '4688', '4714', '4757', '4762', '4764', '4776', '4785', '4797', '4862', '4866', '4873', '4874', '4876', '4881', '4882', '4904', '4912', '4913', '4915', '4918', '4926', '4927', '4929', '4936', '4937', '4944', '4950', '4953', '4962', '4967', '4973', '4994', '5002', '5009', '5051', '5091', '5107', '5117']\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def find_nearby_pois(distance_file, target_pid, threshold):\n",
    "#     \"\"\"\n",
    "#     查找距离指定 PID 小于阈值的其他 POI 的 PID。\n",
    "\n",
    "#     Args:\n",
    "#         distance_file (str): 存储 POI 之间距离的 CSV 文件路径。\n",
    "#         target_pid (int): 要查找附近 POI 的目标 POI 的 PID。\n",
    "#         threshold (float): 距离阈值（单位与 distance_file 中的距离单位一致）。\n",
    "\n",
    "#     Returns:\n",
    "#         list: 包含所有距离目标 PID 小于阈值的其他 POI 的 PID 列表。\n",
    "#               如果目标 PID 不存在于文件中，则返回一个空列表。\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         distance_df = pd.read_csv(distance_file, index_col=0)\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"错误：文件 {distance_file} 未找到。\")\n",
    "#         return []\n",
    "\n",
    "#     if target_pid not in distance_df.index:\n",
    "#         print(f\"警告：目标 PID {target_pid} 不存在于距离文件中。\")\n",
    "#         return []\n",
    "\n",
    "#     # 获取目标 PID 对应的距离Series\n",
    "#     target_distances = distance_df.loc[target_pid]\n",
    "\n",
    "#     # 筛选出距离小于阈值的 PID，并排除目标 PID 本身\n",
    "#     nearby_pois = target_distances[\n",
    "#         (target_distances < threshold) & (distance_df.index != target_pid)\n",
    "#     ].index.tolist()\n",
    "\n",
    "#     return nearby_pois\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     distance_file = 'NYC/distance.csv'  # 替换为你的 distance.csv 文件路径\n",
    "#     target_pid = 0  # 指定你要查找附近 POI 的目标 PID\n",
    "#     distance_threshold = 2.0  # 指定距离阈值，例如 1.0 公里\n",
    "\n",
    "#     nearby_pids = find_nearby_pois(distance_file, target_pid, distance_threshold)\n",
    "\n",
    "#     if nearby_pids:\n",
    "#         print(f\"距离 PID {target_pid} 小于 {distance_threshold} 的 POI PID 有{len(nearby_pids)}个:\")\n",
    "#         print(nearby_pids)\n",
    "#     else:\n",
    "#         print(f\"没有找到距离 PID {target_pid} 小于 {distance_threshold} 的其他 POI。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb33695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2242842/619514515.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grouped = df.groupby('uid').apply(aggregate_and_calculate_distance).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到：TKY/poi_checkin.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "datafold = 'TKY'\n",
    "\n",
    "def process_poi_data(file_path):\n",
    "    \"\"\"\n",
    "    处理POI签到数据，按用户ID分组，并按时间排序POI访问记录。\n",
    "    计算每个POI与前一个POI的距离。\n",
    "\n",
    "    参数:\n",
    "    file_path (str): 包含POI签到数据的文件的路径。\n",
    "\n",
    "    返回:\n",
    "    pandas.DataFrame: 包含处理后数据的DataFrame，其中包含uid、pid_list、category_list、time_list和distance_list列。\n",
    "    \"\"\"\n",
    "    # 读取CSV文件到pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 确保时间列是datetime类型\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # 按uid分组并聚合数据\n",
    "    def aggregate_and_calculate_distance(group):\n",
    "        pid_list = list(group['pid'])\n",
    "        category_list = list(group['category'])\n",
    "        region_list = list(group['region'])\n",
    "        time_list = list(group['time'])\n",
    "        \n",
    "        # 按时间排序\n",
    "        sorted_indices = sorted(range(len(time_list)), key=lambda i: time_list[i])\n",
    "        pid_list = [pid_list[i] for i in sorted_indices]\n",
    "        category_list = [category_list[i] for i in sorted_indices]\n",
    "        region_list = [region_list[i] for i in sorted_indices]\n",
    "        time_list = [time_list[i] for i in sorted_indices]\n",
    "        \n",
    "        # 计算距离\n",
    "        # distance_list = [0.0]  # 第一个POI的距离为0\n",
    "        # for i in range(1, len(latitude_list)):\n",
    "        #     coord1 = (latitude_list[i-1], longitude_list[i-1])\n",
    "        #     coord2 = (latitude_list[i], longitude_list[i])\n",
    "        #     distance = geodesic(coord1, coord2).km  # 使用geodesic计算距离，单位为公里\n",
    "        #     distance_list.append(round(distance, 2))\n",
    "        \n",
    "        return pd.Series({\n",
    "            'pid_list': pid_list,\n",
    "            'category_list': category_list,\n",
    "            'region_list': region_list,\n",
    "            'time_list': [t.strftime('%Y-%m-%d %H:%M') for t in time_list], # 格式化时间\n",
    "            # 'distance_list': distance_list\n",
    "        })\n",
    "\n",
    "    df_grouped = df.groupby('uid').apply(aggregate_and_calculate_distance).reset_index()\n",
    "    return df_grouped\n",
    "\n",
    "\n",
    "def save_processed_data(df, output_file_path):\n",
    "    \"\"\"\n",
    "    将处理后的数据保存到CSV文件。\n",
    "\n",
    "    参数:\n",
    "    df (pandas.DataFrame): 包含处理后数据的DataFrame。\n",
    "    output_file_path (str): 输出文件的路径。\n",
    "    \"\"\"\n",
    "    df.to_csv(output_file_path, index=False, header=True)\n",
    "    print(f\"处理后的数据已保存到：{output_file_path}\")\n",
    "\n",
    "# 指定输入和输出文件路径\n",
    "input_file_path = f'{datafold}/{datafold}.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您想要的输出文件路径\n",
    "\n",
    "# 处理数据\n",
    "processed_df = process_poi_data(input_file_path)\n",
    "\n",
    "# 保存处理后的数据\n",
    "save_processed_data(processed_df, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f85a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转移图已保存到：TKY/poi_transition_graph.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "datafold = 'TKY'\n",
    "\n",
    "def build_poi_transition_graph(file_path):\n",
    "    \"\"\"\n",
    "    根据用户访问序列文件构建POI到POI的转移图。\n",
    "\n",
    "    参数:\n",
    "    file_path (str): 包含用户访问序列数据的文件路径。\n",
    "\n",
    "    返回:\n",
    "    dict: 一个字典，表示POI到POI的转移图。\n",
    "          键是POI ID，值是潜在的后续POI ID列表。\n",
    "    \"\"\"\n",
    "    # 读取CSV文件到pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 初始化转移图\n",
    "    transition_graph = defaultdict(list)\n",
    "\n",
    "    df['poi_list'] = df['pid_list'].apply(lambda x: eval(x))  # 将字符串转换为列表\n",
    "    user_checkin = df['poi_list'].tolist()\n",
    "\n",
    "    for i in range(len(user_checkin)):\n",
    "        # 获取当前用户的POI列表\n",
    "        poi_list = user_checkin[i]\n",
    "        # 遍历当前用户访问的POI列表\n",
    "        for j in range(len(poi_list) - 1):\n",
    "            current_poi = poi_list[j]\n",
    "            next_poi = poi_list[j + 1]\n",
    "            # 将下一个POI添加到当前POI的潜在后续POI列表中\n",
    "            if next_poi not in transition_graph[current_poi]:\n",
    "                transition_graph[current_poi].append(next_poi)\n",
    "\n",
    "    return transition_graph\n",
    "\n",
    "def save_transition_graph(graph, output_file_path):\n",
    "    \"\"\"\n",
    "    将POI到POI的转移图保存到CSV文件。\n",
    "\n",
    "    参数:\n",
    "    graph (dict): POI到POI的转移图。\n",
    "    output_file_path (str): 输出文件的路径。\n",
    "    \"\"\"\n",
    "    # 将字典转换为DataFrame\n",
    "    df = pd.DataFrame(list(graph.items()), columns=['pid', 'potential_poi'])\n",
    "    # 保存到CSV文件\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"转移图已保存到：{output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定输入和输出文件路径\n",
    "    input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "    output_file_path = f'{datafold}/poi_transition_graph.csv'  # 替换为您想要的输出文件路径\n",
    "\n",
    "    # 构建转移图\n",
    "    transition_graph = build_poi_transition_graph(input_file_path)\n",
    "\n",
    "    # 保存转移图\n",
    "    save_transition_graph(transition_graph, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91232bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: NYC/data.json\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "# import json\n",
    "# import ast\n",
    "\n",
    "\n",
    "# datafold = 'NYC'\n",
    "# def convert_csv_to_json(input_csv_path, output_json_path):\n",
    "#     data = []\n",
    "\n",
    "#     with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             uid = int(row['uid'])\n",
    "#             pid_list = ast.literal_eval(row['pid_list'])\n",
    "#             category_list = ast.literal_eval(row['category_list'])\n",
    "#             region_list = ast.literal_eval(row['region_list'])\n",
    "#             time_list = ast.literal_eval(row['time_list'])\n",
    "#             # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            \n",
    "#             max_length = 50\n",
    "#             record = {\n",
    "#                 \"uid\": uid,\n",
    "#                 \"pid_list\": pid_list[-max_length:] if len(pid_list) > max_length else pid_list,\n",
    "#                 \"category_list\": category_list[-max_length:] if len(category_list) > max_length else category_list,\n",
    "#                 \"region_list\": region_list[-max_length:] if len(region_list) > max_length else region_list,\n",
    "#                 \"time_list\": time_list[-max_length:] if len(time_list) > max_length else time_list,\n",
    "#                 # \"distance_list\": distance_list[-max_length:] if len(distance_list) > max_length else distance_list\n",
    "#             }\n",
    "\n",
    "#             data.append(record)\n",
    "\n",
    "#     with open(output_json_path, mode='w', encoding='utf-8') as jsonfile:\n",
    "#         jsonfile.write('[\\n')\n",
    "#         for i, record in enumerate(data):\n",
    "#             line = json.dumps(record, ensure_ascii=False, separators=(',', ': '))\n",
    "#             jsonfile.write('  ' + line)\n",
    "#             if i < len(data) - 1:\n",
    "#                 jsonfile.write(',\\n')\n",
    "#             else:\n",
    "#                 jsonfile.write('\\n')\n",
    "#         jsonfile.write(']\\n')\n",
    "\n",
    "#     print(f\"转换完成，结果保存至: {output_json_path}\")\n",
    "# # 示例用法\n",
    "# input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "# output_file_path = f'{datafold}/data.json'  # 替换为您想要的输出文件路径\n",
    "# convert_csv_to_json(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果保存至: NYC/data50.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "datafold = 'TKY'\n",
    "max_length = 50\n",
    "\n",
    "def convert_csv_to_json(input_csv_path, output_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            uid = int(row['uid'])\n",
    "            pid_list = ast.literal_eval(row['pid_list'])\n",
    "            # category_list = ast.literal_eval(row['category_list'])\n",
    "            # region_list = ast.literal_eval(row['region_list'])\n",
    "            time_list = ast.literal_eval(row['time_list'])\n",
    "            # distance_list = ast.literal_eval(row['distance_list'])\n",
    "            record = {\n",
    "                \"uid\": uid,\n",
    "                \"history\": pid_list[-max_length-1:-1] if len(pid_list) > max_length else pid_list,\n",
    "                \"time\": time_list[-max_length-1:-1] if len(time_list) > max_length else time_list,\n",
    "                \"next_time\": time_list[-1],\n",
    "                \"target_pid\": pid_list[-1]\n",
    "            }\n",
    "\n",
    "            data.append(record)\n",
    "\n",
    "    json_records = []\n",
    "    for record in data:\n",
    "        uid = record[\"uid\"]\n",
    "        history = record[\"history\"]\n",
    "        time_seq = record[\"time\"]\n",
    "        next_time = record[\"next_time\"]\n",
    "        target_pid = record[\"target_pid\"]\n",
    "\n",
    "            # 构造 input 字符串\n",
    "        input_text = (\n",
    "            f\"The historical POI check-in records for user {uid} are as follows:\\n\"\n",
    "            f\"POI list: {history}, with corresponding check-in times: {time_seq}.\"\n",
    "            # f\"At {next_time}, which POI is the user most likely to check in at?\"\n",
    "        )\n",
    "\n",
    "        record = {\n",
    "            \"input\": input_text,\n",
    "            \"next_time\": next_time,\n",
    "            \"target\": target_pid\n",
    "        }\n",
    "        json_records.append(record)\n",
    "\n",
    "\n",
    "    with open(output_file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(json_records, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"转换完成，结果保存至: {output_file_path}\")\n",
    "# 示例用法\n",
    "input_file_path = f'{datafold}/poi_checkin.csv'  # 替换为您的输入文件路径\n",
    "output_file_path = f'{datafold}/data{max_length}.json'  # 替换为您想要的输出文件路径\n",
    "convert_csv_to_json(input_file_path, output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poiRe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
